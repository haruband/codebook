대표적인 빅데이터 처리 기술인 스파크(Spark)는 중간 결과를 하둡(Hadoop)처럼 디스크에 저장하지 않고 메모리에만 저장함으로써 놀랄만한 성능 개선을 가져왔다. 하지만 그만큼 메모리 사용량이 늘어났고 다양한 상황에서 메모리 부족 현상(OOM)이 발생한다. 이는 쿠버네티스(K8S) 환경에서 다양한 서비스들과 스파크를 동시에 운영할 때 더 다루기 힘든 상황이며, 오늘은 최근 쿠버네티스에서도 지원하기 시작한 스왑(Swap)을 이용하여 이러한 문제를 어떻게 해결했는지 소개하고자 한다.

우선 쿠버네티스 환경에서 스파크를 운영할 때 메모리 관련해서 발생하는 다양한 문제들에 대해 살펴보자.

## JVM OOM(Out-Of-Memory)

스칼라(Scala)로 개발된 스파크는 JVM 위에서 동작하기 때문에 JVM 이 사용할 최대 힙(Heap)의 크기를 지정해야 한다. 이 값이 실제 최대 사용량보다 작으면 OOM(JVM) 에 의해 해당 작업이 종료되고, 너무 크면 낭비되는 메모리가 많아질 수 있으니 적절한 값으로 설정하는 것이 중요하다.

## CGroup OOM(Out-Of-Memory)

스파크의 쿠버네티스 스케줄러는 익스큐터(Executor) 파드(Pod)에 JVM 최대 힙 크기와 메모리 오버헤드 크기 등을 합쳐서 요청량(Requests)과 제한량(Limits)을 적용한다. 만약 해당 파드가 스왑 없이 익명 페이지를 메모리 한도보다 많이 사용할 경우, OOM(CGroup) 에 의해 해당 파드가 종료된다. 파드의 메모리 사용량이 한도보다 많아지면 지속적으로 메모리를 반환하지만, 스왑이 없으면 익명 페이지는 반환되지 않기 때문이다.

## Process OOM(Out-Of-Memory)

시스템의 물리 메모리가 부족해지면 커널에 의해 우선순위를 따라서 특정 프로세스가 종료(OOM)되기 시작한다. 쿠버네티스는 QoS 정책에 따라 우선순위를 지정하기 때문에, 서비스의 중요도에 따라서 QoS 정책을 잘 세워야만 최악의 상황은 면할 수 있다.

## K8S PodEviction

시스템의 물리 메모리가 지정된 값보다 작아지면, Kubelet 은 주기적으로 우선순위가 낮은 파드를 종료시킨다. 기본 설정값이 10 초마다 물리 메모리가 100 MBytes 이하일 때 파드를 추출하는 것인데, 우리가 실험 환경에서는 Kubelet 에 의해 파드가 추출되는 경우는 한번도 없었고 모두 커널에 의해 OOM 으로 종료되었다. 기본 설정값을 변경하거나 실험 환경이 다르면 다른 결과를 보일 수도 있겠지만, 현실적으로 실효성이 있는지는 의문이다.
